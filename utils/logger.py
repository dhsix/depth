import logging
import os
import sys
import json
import yaml
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

def setup_logger(name: str, 
                log_file: Optional[str] = None, 
                level: int = logging.INFO,
                console_output: bool = True) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    
    # åˆ›å»ºlogger
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # é¿å…é‡å¤æ·»åŠ handler
    if logger.handlers:
        return logger
    
    # åˆ›å»ºformatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # æ§åˆ¶å°è¾“å‡º
    if console_output:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(level)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
    
    # æ–‡ä»¶è¾“å‡º - å¯ç”¨æ–‡ä»¶è®°å½•
    if log_file:
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger

class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨ - å¢å¼ºç‰ˆ"""
    
    def __init__(self, log_dir: str, experiment_name: str):
        self.log_dir = Path(log_dir)
        self.experiment_name = experiment_name
        
        # åˆ›å»ºå®éªŒç›®å½•
        self.experiment_dir = self.log_dir / experiment_name
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = self.experiment_dir / f"training_{timestamp}.log"
        
        self.logger = setup_logger(
            name=f"training_{experiment_name}",
            log_file=str(log_file),
            level=logging.INFO
        )
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡
        self.metrics_history = {
            'train_loss': [],
            'val_loss': [],
            'epoch': []
        }
        
        # è®°å½•batchçº§åˆ«çš„æŒ‡æ ‡
        self.batch_metrics_history = []
        
        # é…ç½®ä¿¡æ¯å­˜å‚¨
        self.config_info = None
        
    def log_config(self, config):
        """è®°å½•é…ç½®ä¿¡æ¯ - å¢å¼ºç‰ˆæœ¬ï¼Œæ›´å¥½åœ°å¤„ç†yamlé…ç½®"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸš€ EXPERIMENT CONFIGURATION")
        self.logger.info("=" * 80)
        
        # å¤„ç†ä¸åŒç±»å‹çš„é…ç½®
        if hasattr(config, 'to_dict'):
            config_dict = config.to_dict()
        elif isinstance(config, dict):
            config_dict = config
        else:
            config_dict = vars(config) if hasattr(config, '__dict__') else str(config)
        
        # å­˜å‚¨é…ç½®ä¿¡æ¯
        self.config_info = config_dict
        
        # åˆ†ç±»æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        self._log_config_section("ğŸ“Š Data Configuration", config_dict, [
            'data_root', 'dataset_name', 'input_size', 'patch_size', 
            'batch_size', 'num_workers'
        ])
        
        self._log_config_section("ğŸ§  Model Configuration", config_dict, [
            'model_name', 'model_config', 'encoder', 'embed_dim', 'num_heads'
        ])
        
        self._log_config_section("ğŸ¯ Training Configuration", config_dict, [
            'num_epochs', 'learning_rate', 'weight_decay', 'optimizer', 
            'scheduler', 'val_interval', 'save_interval'
        ])
        
        self._log_config_section("ğŸ“‰ Loss Configuration", config_dict, [
            'loss_config', 'trainer_type', 'use_multi_scale_output'
        ])
        
        # å¦‚æœæœ‰åˆ†å¸ƒé‡åŠ æƒé…ç½®
        if 'reweight_config' in config_dict and config_dict['reweight_config'].get('enable', False):
            self._log_config_section("âš–ï¸ Distribution Reweighting Configuration", 
                                    config_dict, ['reweight_config'])
        
        self._log_config_section("ğŸ”§ System Configuration", config_dict, [
            'device', 'seed', 'experiment_name', 'checkpoint_dir', 'log_dir'
        ])
        
        # è®°å½•å…¶ä»–é…ç½®
        other_keys = set(config_dict.keys()) - {
            'data_root', 'dataset_name', 'input_size', 'patch_size', 'batch_size', 
            'num_workers', 'model_name', 'model_config', 'num_epochs', 'learning_rate', 
            'weight_decay', 'optimizer', 'scheduler', 'val_interval', 'save_interval',
            'loss_config', 'trainer_type', 'use_multi_scale_output', 'reweight_config',
            'device', 'seed', 'experiment_name', 'checkpoint_dir', 'log_dir'
        }
        
        if other_keys:
            self._log_config_section("ğŸ“ Other Configuration", config_dict, list(other_keys))
        
        self.logger.info("=" * 80)
        
        # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
        self._save_config_to_file(config_dict)
        
    def _log_config_section(self, section_title: str, config_dict: Dict[str, Any], keys: list):
        """è®°å½•é…ç½®çš„æŸä¸ªéƒ¨åˆ†"""
        self.logger.info(f"\n{section_title}:")
        self.logger.info("-" * 40)
        
        for key in keys:
            if key in config_dict:
                value = config_dict[key]
                if isinstance(value, dict):
                    self.logger.info(f"  {key}:")
                    for sub_key, sub_value in value.items():
                        self.logger.info(f"    {sub_key}: {sub_value}")
                else:
                    self.logger.info(f"  {key}: {value}")
    
    def _save_config_to_file(self, config_dict: Dict[str, Any]):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        config_json_file = self.experiment_dir / "experiment_config.json"
        with open(config_json_file, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False, default=str)
        
        # å¦‚æœå¯èƒ½ï¼Œä¹Ÿä¿å­˜ä¸ºYAMLæ ¼å¼
        try:
            config_yaml_file = self.experiment_dir / "experiment_config.yaml"
            with open(config_yaml_file, 'w', encoding='utf-8') as f:
                yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
        except Exception as e:
            self.logger.warning(f"Failed to save config as YAML: {e}")
        
        self.logger.info(f"ğŸ’¾ Configuration saved to: {config_json_file}")
    
    def log_batch_progress(self, epoch: int, batch_idx: int, total_batches: int, 
                          metrics: Dict[str, float], lr: float = None):
        """è®°å½•batchçº§åˆ«çš„è®­ç»ƒè¿›åº¦"""
        # æ¯100ä¸ªbatchæˆ–æœ€åå‡ ä¸ªbatchè®°å½•ä¸€æ¬¡
        if batch_idx % 100 == 0 or batch_idx >= total_batches - 5:
            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            progress = (batch_idx + 1) / total_batches * 100
            
            # æ ¼å¼åŒ–æŒ‡æ ‡
            metrics_str = ", ".join([f"{k}: {v:.4f}" for k, v in metrics.items()])
            
            # å­¦ä¹ ç‡ä¿¡æ¯
            lr_str = f", LR: {lr:.2e}" if lr is not None else ""
            
            # è¿›åº¦æ¡
            bar_length = 30
            filled_length = int(bar_length * (batch_idx + 1) // total_batches)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
            
            log_message = (
                f"ğŸ“ˆ Epoch {epoch:3d} | "
                f"[{bar}] {progress:6.2f}% | "
                f"Batch {batch_idx + 1:4d}/{total_batches:4d} | "
                f"{metrics_str}{lr_str}"
            )
            
            self.logger.info(log_message)
        
        # è®°å½•batchçº§åˆ«çš„æŒ‡æ ‡å†å²ï¼ˆé‡‡æ ·è®°å½•ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§ï¼‰
        if batch_idx % 100 == 0:
            batch_record = {
                'epoch': epoch,
                'batch_idx': batch_idx,
                'metrics': metrics.copy(),
                'lr': lr,
                'timestamp': datetime.now().isoformat()
            }
            self.batch_metrics_history.append(batch_record)
    
    def log_epoch(self, epoch: int, train_loss: float, val_loss: float, **kwargs):
        """è®°å½•æ¯ä¸ªepochçš„ç»“æœ - æ·±åº¦ä¼°è®¡ä¸“ç”¨å¢å¼ºç‰ˆæœ¬"""
        self.metrics_history['epoch'].append(epoch)
        self.metrics_history['train_loss'].append(train_loss)
        self.metrics_history['val_loss'].append(val_loss)
        
        # è®°å½•é¢å¤–æŒ‡æ ‡
        for key, value in kwargs.items():
            if key not in self.metrics_history:
                self.metrics_history[key] = []
            self.metrics_history[key].append(value)
        
        # æ ¼å¼åŒ–è¾“å‡º
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸ EPOCH {epoch:3d} COMPLETED")
        self.logger.info("=" * 80)
        self.logger.info(f"ğŸ“Š Train Loss: {train_loss:.6f}")
        if not (isinstance(val_loss, float) and (val_loss != val_loss)):  # æ£€æŸ¥ä¸æ˜¯NaN
            self.logger.info(f"ğŸ“Š Val Loss:   {val_loss:.6f}")
        
        # ä¸“é—¨å¤„ç†æ·±åº¦ä¼°è®¡æŒ‡æ ‡
        if kwargs:
            self._log_depth_metrics(kwargs)
        
        self.logger.info("=" * 80)
    
    def _log_depth_metrics(self, metrics: Dict[str, Any]):
        """ä¸“é—¨è®°å½•æ·±åº¦ä¼°è®¡æŒ‡æ ‡"""
        # åˆ†ç±»æ•´ç†æŒ‡æ ‡
        loss_metrics = {}
        depth_metrics = {}
        accuracy_metrics = {}
        other_metrics = {}
        
        for key, value in metrics.items():
            if 'loss' in key.lower():
                loss_metrics[key] = value
            elif key in ['MAE', 'RMSE', 'SI_RMSE', 'AbsRel', 'SqRel']:
                depth_metrics[key] = value
            elif key.startswith('Î´') or 'accuracy' in key.lower():
                accuracy_metrics[key] = value
            else:
                other_metrics[key] = value
        
        # æ˜¾ç¤ºæŸå¤±ç›¸å…³æŒ‡æ ‡
        if loss_metrics:
            self.logger.info("ğŸ’¸ Loss Breakdown:")
            for key, value in loss_metrics.items():
                if isinstance(value, (int, float)) and not (isinstance(value, float) and (value != value)):
                    self.logger.info(f"   {key}: {value:.6f}")
                else:
                    self.logger.info(f"   {key}: {value}")
        
        # æ˜¾ç¤ºæ·±åº¦ä¼°è®¡æ ¸å¿ƒæŒ‡æ ‡
        if depth_metrics:
            self.logger.info("ğŸ“ Depth Estimation Metrics:")
            # æŒ‰é‡è¦æ€§æ’åºæ˜¾ç¤º
            metric_order = ['MAE', 'RMSE', 'SI_RMSE', 'AbsRel', 'SqRel']
            for metric in metric_order:
                if metric in depth_metrics:
                    value = depth_metrics[metric]
                    if isinstance(value, (int, float)) and not (isinstance(value, float) and (value != value)):
                        # æ ¹æ®æŒ‡æ ‡ç±»å‹é€‰æ‹©åˆé€‚çš„ç²¾åº¦
                        if metric in ['MAE', 'RMSE', 'SI_RMSE']:
                            self.logger.info(f"   ğŸ“ {metric}: {value:.4f}")
                        else:  # AbsRel, SqRel
                            self.logger.info(f"   ğŸ“Š {metric}: {value:.4f}")
            
            # æ˜¾ç¤ºå‰©ä½™çš„æ·±åº¦æŒ‡æ ‡
            for key, value in depth_metrics.items():
                if key not in metric_order:
                    if isinstance(value, (int, float)) and not (isinstance(value, float) and (value != value)):
                        self.logger.info(f"   ğŸ“Š {key}: {value:.4f}")
        
        # æ˜¾ç¤ºå‡†ç¡®ç‡æŒ‡æ ‡
        if accuracy_metrics:
            self.logger.info("ğŸ¯ Accuracy Metrics:")
            for key, value in accuracy_metrics.items():
                if isinstance(value, (int, float)) and not (isinstance(value, float) and (value != value)):
                    # å‡†ç¡®ç‡é€šå¸¸æ˜¯0-1ä¹‹é—´ï¼Œæ˜¾ç¤ºä¸ºç™¾åˆ†æ¯”
                    if 0 <= value <= 1:
                        self.logger.info(f"   âœ… {key}: {value:.1%}")
                    else:
                        self.logger.info(f"   âœ… {key}: {value:.4f}")
        
        # æ˜¾ç¤ºå…¶ä»–æŒ‡æ ‡
        if other_metrics:
            self.logger.info("ğŸ“ˆ Other Metrics:")
            for key, value in other_metrics.items():
                if isinstance(value, (int, float)) and not (isinstance(value, float) and (value != value)):
                    self.logger.info(f"   ğŸ“Š {key}: {value:.4f}")
                else:
                    self.logger.info(f"   ğŸ“Š {key}: {value}")
    
    def log_depth_summary(self, metrics: Dict[str, float]):
        """è®°å½•æ·±åº¦ä¼°è®¡æŒ‡æ ‡æ‘˜è¦"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š DEPTH ESTIMATION PERFORMANCE SUMMARY")
        self.logger.info("=" * 80)
        
        # æ ¸å¿ƒæŒ‡æ ‡æ‘˜è¦
        core_metrics = ['MAE', 'RMSE', 'SI_RMSE']
        self.logger.info("ğŸ¯ Core Performance Metrics:")
        for metric in core_metrics:
            if metric in metrics:
                value = metrics[metric]
                if isinstance(value, (int, float)):
                    self.logger.info(f"   ğŸ“ {metric}: {value:.4f}")
        
        # ç›¸å¯¹è¯¯å·®æŒ‡æ ‡
        rel_metrics = ['AbsRel', 'SqRel']
        if any(m in metrics for m in rel_metrics):
            self.logger.info("ğŸ“Š Relative Error Metrics:")
            for metric in rel_metrics:
                if metric in metrics:
                    value = metrics[metric]
                    if isinstance(value, (int, float)):
                        self.logger.info(f"   ğŸ“Š {metric}: {value:.4f}")
        
        # é˜ˆå€¼å‡†ç¡®ç‡
        threshold_metrics = [k for k in metrics.keys() if k.startswith('Î´')]
        if threshold_metrics:
            self.logger.info("ğŸ¯ Threshold Accuracy:")
            for metric in sorted(threshold_metrics):
                value = metrics[metric]
                if isinstance(value, (int, float)):
                    self.logger.info(f"   âœ… {metric}: {value:.1%}")
        
        self.logger.info("=" * 80)
    
    def log_info(self, message: str):
        """è®°å½•ä¿¡æ¯"""
        self.logger.info(message)
    
    def log_warning(self, message: str):
        """è®°å½•è­¦å‘Š"""
        self.logger.warning(f"âš ï¸  {message}")
    
    def log_error(self, message: str):
        """è®°å½•é”™è¯¯"""
        self.logger.error(f"âŒ {message}")
    
    def log_success(self, message: str):
        """è®°å½•æˆåŠŸä¿¡æ¯"""
        self.logger.info(f"âœ… {message}")
    
    def log_model_info(self, model_info: Dict[str, Any]):
        """è®°å½•æ¨¡å‹ä¿¡æ¯"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ—ï¸  MODEL INFORMATION")
        self.logger.info("=" * 80)
        
        for key, value in model_info.items():
            if key == 'total_parameters' or key == 'trainable_parameters':
                # æ ¼å¼åŒ–å‚æ•°æ•°é‡
                if isinstance(value, int):
                    value_str = f"{value:,} ({value/1e6:.2f}M)" if value > 1e6 else f"{value:,}"
                    self.logger.info(f"  {key}: {value_str}")
                else:
                    self.logger.info(f"  {key}: {value}")
            else:
                self.logger.info(f"  {key}: {value}")
        
        self.logger.info("=" * 80)
    
    def log_training_start(self, total_epochs: int, train_batches: int, val_batches: int):
        """è®°å½•è®­ç»ƒå¼€å§‹ä¿¡æ¯"""
        self.logger.info("ğŸš€" * 20)
        self.logger.info("ğŸš€ TRAINING STARTED")
        self.logger.info("ğŸš€" * 20)
        self.logger.info(f"ğŸ“… Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"ğŸ¯ Total Epochs: {total_epochs}")
        self.logger.info(f"ğŸ“Š Train Batches: {train_batches}")
        self.logger.info(f"ğŸ“Š Val Batches: {val_batches}")
        self.logger.info("ğŸš€" * 20)
    
    def log_training_complete(self, total_time: float, best_val_loss: float = None):
        """è®°å½•è®­ç»ƒå®Œæˆä¿¡æ¯"""
        self.logger.info("ğŸ‰" * 20)
        self.logger.info("ğŸ‰ TRAINING COMPLETED")
        self.logger.info("ğŸ‰" * 20)
        self.logger.info(f"ğŸ“… End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"â±ï¸  Total Time: {total_time/3600:.2f} hours")
        if best_val_loss is not None:
            self.logger.info(f"ğŸ† Best Val Loss: {best_val_loss:.6f}")
        self.logger.info("ğŸ‰" * 20)
    
    def log_dataset_info(self, train_size: int, val_size: int, test_size: int = None):
        """è®°å½•æ•°æ®é›†ä¿¡æ¯"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“‚ DATASET INFORMATION")
        self.logger.info("=" * 80)
        self.logger.info(f"  ğŸ“Š Train Samples: {train_size:,}")
        self.logger.info(f"  ğŸ“Š Val Samples: {val_size:,}")
        if test_size is not None:
            self.logger.info(f"  ğŸ“Š Test Samples: {test_size:,}")
        self.logger.info(f"  ğŸ“Š Total Samples: {train_size + val_size + (test_size or 0):,}")
        self.logger.info("=" * 80)
    
    def log_checkpoint_saved(self, epoch: int, path: str, is_best: bool = False):
        """è®°å½•æ£€æŸ¥ç‚¹ä¿å­˜"""
        checkpoint_type = "ğŸ† BEST" if is_best else "ğŸ’¾"
        self.logger.info(f"{checkpoint_type} Checkpoint saved at epoch {epoch}: {path}")
    
    def save_metrics(self):
        """ä¿å­˜è®­ç»ƒæŒ‡æ ‡"""
        # ä¿å­˜epochçº§åˆ«çš„æŒ‡æ ‡
        metrics_file = self.experiment_dir / "training_metrics.json"
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(self.metrics_history, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜batchçº§åˆ«çš„æŒ‡æ ‡
        if self.batch_metrics_history:
            batch_metrics_file = self.experiment_dir / "batch_metrics.json"
            with open(batch_metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.batch_metrics_history, f, indent=2, ensure_ascii=False, default=str)
        
        self.log_success(f"Metrics saved to {self.experiment_dir}")
    
    def get_summary(self) -> str:
        """è·å–è®­ç»ƒæ‘˜è¦"""
        if not self.metrics_history['train_loss']:
            return "No training data available"
        
        best_epoch = self.metrics_history['val_loss'].index(min(self.metrics_history['val_loss']))
        best_train_loss = self.metrics_history['train_loss'][best_epoch]
        best_val_loss = self.metrics_history['val_loss'][best_epoch]
        
        summary = f"""
        ğŸ“‹ TRAINING SUMMARY
        ==================
        ğŸ¯ Total Epochs: {len(self.metrics_history['train_loss'])}
        ğŸ† Best Epoch: {best_epoch + 1}
        ğŸ“‰ Best Train Loss: {best_train_loss:.6f}
        ğŸ“‰ Best Val Loss: {best_val_loss:.6f}
        """
        return summary