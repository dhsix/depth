import torch
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from typing import Dict, Any, Tuple, Optional
from .datasets import get_dataset
from .transforms import get_transforms

def collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°"""
    keys = batch[0].keys()
    result = {}
    
    for key in keys:
        if key in ['image', 'depth', 'mask','semantic_mask']:
            # æ•°å€¼æ•°æ®ï¼Œç›´æ¥stack
            result[key] = torch.stack([item[key] for item in batch])
        else:
            # è·¯å¾„ç­‰å…ƒæ•°æ®ï¼Œä¿æŒä¸ºåˆ—è¡¨
            result[key] = [item[key] for item in batch]
    
    return result

def create_data_loaders(config) -> Tuple[DataLoader, DataLoader, Optional[DataLoader]]:
    """åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    # ğŸ”¥ ä»é…ç½®ä¸­è·å–æ˜¯å¦åŠ è½½mask
    load_mask = getattr(config, 'load_mask', True)
    # è·å–æ•°æ®å˜æ¢
    train_transform = get_transforms(
        {
            'input_size': config.input_size,
            'patch_size': config.patch_size,
            **config.augmentation_config
        }, 
        is_training=True
    )
    
    val_transform = get_transforms(
        {
            'input_size': config.input_size,
            'patch_size': config.patch_size
        }, 
        is_training=False
    )
    
    # è·å–å®Œæ•´çš„æ•°æ®é›†è·¯å¾„

    dataset_path=config.data_root

    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = get_dataset(
        dataset_name=config.dataset_name,
        data_root=dataset_path,  # ä¼ å…¥å®Œæ•´è·¯å¾„
        split='train',
        transform=train_transform,
        load_mask=load_mask
    )
    
    val_dataset = get_dataset(
        dataset_name=config.dataset_name,
        data_root=dataset_path,  # ä¼ å…¥å®Œæ•´è·¯å¾„
        split='val',
        transform=val_transform,
        load_mask=load_mask
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        sampler=RandomSampler(train_dataset),
        num_workers=config.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        sampler=SequentialSampler(val_dataset),
        num_workers=config.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=False
    )
    
    # å¯é€‰çš„æµ‹è¯•é›†
    test_loader = None
    try:
        test_dataset = get_dataset(
            dataset_name=config.dataset_name,
            data_root=dataset_path,  # ä½¿ç”¨å®Œæ•´è·¯å¾„
            split='test',
            transform=val_transform,
            load_mask=load_mask
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=config.batch_size,
            sampler=SequentialSampler(test_dataset),
            num_workers=config.num_workers,
            collate_fn=collate_fn,
            pin_memory=True,
            drop_last=False
        )
    except:
        print("No test dataset found, skipping...")
    
    print(f"Created data loaders:")
    print(f"  Train: {len(train_loader)} batches, {len(train_dataset)} samples")
    print(f"  Val: {len(val_loader)} batches, {len(val_dataset)} samples")
    if test_loader:
        print(f"  Test: {len(test_loader)} batches, {len(test_dataset)} samples")
    
    return train_loader, val_loader, test_loader

def get_sample_batch(config, split: str = 'train') -> Dict[str, Any]:
    """è·å–ä¸€ä¸ªæ ·æœ¬batchï¼Œç”¨äºè°ƒè¯•"""
    transform = get_transforms(
        {
            'input_size': config.input_size,
            'patch_size': config.patch_size
        }, 
        is_training=False
    )
    load_mask = getattr(config, 'load_mask', True)
    dataset = get_dataset(
        dataset_name=config.dataset_name,
        data_root=config.data_root,
        split=split,
        transform=transform,
        load_mask=load_mask
    )
    
    loader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)
    return next(iter(loader))