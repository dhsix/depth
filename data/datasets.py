import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset
from typing import Dict, Any, Optional, Callable, Tuple
import glob
from pathlib import Path

class BaseDepthDataset(Dataset):
    """åŸºç¡€æ·±åº¦ä¼°è®¡æ•°æ®é›†"""
    
    def __init__(self, 
                 data_root: str,
                 split: str = 'train',
                 transform: Optional[Callable] = None,
                 load_mask: bool = False):
        self.data_root = Path(data_root)
        self.split = split
        self.transform = transform
        self.load_mask = load_mask
        
        self.samples = self._load_samples()
        
    def _load_samples(self) -> list:
        """åŠ è½½æ ·æœ¬åˆ—è¡¨ï¼Œå­ç±»éœ€è¦å®ç°"""
        raise NotImplementedError
    
    def _load_image(self, image_path: str) -> np.ndarray:
        """åŠ è½½å›¾åƒ"""
        image = cv2.imread(image_path)
        if image is None:
            raise FileNotFoundError(f"Cannot load image: {image_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        return image.astype(np.float32) / 255.0
    
    def _load_depth(self, depth_path: str) -> np.ndarray:
        """åŠ è½½æ·±åº¦å›¾ï¼Œå­ç±»å¯ä»¥é‡å†™"""
        depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)
        if depth is None:
            raise FileNotFoundError(f"Cannot load depth: {depth_path}")
            # å‡è®¾ä½ çš„ depth æ˜¯ 16 ä½ï¼Œå•ä½æ˜¯ cm æˆ– dmï¼Œè½¬æˆ m èŒƒå›´æ˜¯åˆç†çš„
        # å¦‚æœæ˜¯ 8 ä½ï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„
        if depth.dtype == np.uint16:
            depth = depth / 1000.0  # è½¬æˆä»¥ç±³ä¸ºå•ä½ï¼Œå‡å¦‚ä½ çš„å•ä½æ˜¯ mm
        elif depth.dtype == np.uint8:
            depth = depth / 255.0  # å½’ä¸€åŒ–
        return depth
        # return depth.astype(np.float32)
    
    def __len__(self) -> int:
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        sample_info = self.samples[idx]
        
        # åŠ è½½å›¾åƒå’Œæ·±åº¦
        image = self._load_image(sample_info['image_path'])
        depth = self._load_depth(sample_info['depth_path'])
        
        sample = {
            'image': image,
            'depth': depth,
            'image_path': sample_info['image_path'],
            'depth_path': sample_info['depth_path']
        }
        
        # å¯é€‰åŠ è½½mask
        if self.load_mask and 'mask_path' in sample_info:
            mask = cv2.imread(sample_info['mask_path'], cv2.IMREAD_GRAYSCALE)
            sample['mask'] = mask.astype(np.float32) / 255.0
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            sample = self.transform(sample)
        
        return sample

class GAMUSDataset(BaseDepthDataset):
    """GAMUSæ•°æ®é›†"""
    def __init__(self, *args, **kwargs):
        # GAMUSè¯­ä¹‰ç±»åˆ«å®šä¹‰
        self.HEIGHT_CLASSES = [3, 6]  # Low-vegetation, Building, Tree
        self.EXCLUDE_CLASSES = [0, 1, 2, 4, 5]  # Background, Ground, Water, Road
        super().__init__(*args, **kwargs)
    def _load_samples(self) -> list:
        # ä½¿ç”¨å®Œæ•´çš„æ•°æ®é›†è·¯å¾„
        dataset_root = self.data_root  # è¿™é‡Œdata_rootåº”è¯¥æ˜¯å®Œæ•´è·¯å¾„ï¼Œå¦‚ "./datasets/GAMUS"
        split_dir = Path(dataset_root) / self.split
        
        # æ•°æ®ç»“æ„ï¼š
        # datasets/GAMUS/
        #   train/
        #     images/
        #     depths/
        #   val/
        #   test/
        
        image_dir = split_dir / "images"
        depth_dir = split_dir / "depths"
        mask_dir= split_dir / "masks"
        
        if not image_dir.exists():
            raise FileNotFoundError(f"Image directory not found: {image_dir}")
        if not depth_dir.exists():
            raise FileNotFoundError(f"Depth directory not found: {depth_dir}")
            # æ£€æŸ¥maskç›®å½•æ˜¯å¦å­˜åœ¨
        mask_available = mask_dir.exists()
        if self.load_mask and not mask_available:
            print(f"Warning: load_mask=True but mask directory not found: {mask_dir}")
            print("Continuing without masks...")
            self.load_mask = False

        if mask_available:
            print(f"Found mask directory: {mask_dir}")
        samples = []
        for image_path in sorted(image_dir.glob("*.jpg")):
            # å¯¹åº”çš„æ·±åº¦æ–‡ä»¶
            # depth_path = depth_dir / f"{image_path.stem}.tif"
            depth_filename=image_path.name.replace("RGB","AGL").replace(".jpg",".png")
            depth_path=depth_dir/depth_filename
            
            if depth_path.exists():
                sample_info = {
                    'image_path': str(image_path),
                    'depth_path': str(depth_path)
                }
                
                # å¦‚æœéœ€è¦åŠ è½½maskï¼Œæ·»åŠ maskè·¯å¾„
                if self.load_mask and mask_available:
                    mask_filename = image_path.name.replace("RGB", "CLS").replace(".jpg", ".png")
                    mask_path = mask_dir / mask_filename
                    
                    if mask_path.exists():
                        sample_info['mask_path'] = str(mask_path)
                    else:
                        print(f"Warning: mask file not found: {mask_path}")
                        # å¯ä»¥é€‰æ‹©è·³è¿‡è¿™ä¸ªæ ·æœ¬ï¼Œæˆ–è€…è®¾ç½®mask_pathä¸ºNone
                        continue  # è·³è¿‡æ²¡æœ‰maskçš„æ ·æœ¬
                
                samples.append(sample_info)
        
        print(f"Loaded {len(samples)} samples from {split_dir}")
        if self.load_mask:
            mask_count = sum(1 for s in samples if 'mask_path' in s)
            print(f"  - {mask_count} samples have masks")
        return samples
    def _load_depth(self, depth_path: str) -> np.ndarray:
        """åŠ è½½GAMUSæ·±åº¦å›¾ - ç‰¹æ®Šå¤„ç†cmå•ä½"""
        depth = cv2.imread(depth_path, cv2.IMREAD_UNCHANGED)
        if depth is None:
            raise FileNotFoundError(f"Cannot load depth: {depth_path}")
        
        # GAMUSæ•°æ®é›†ç‰¹æ®Šå¤„ç†ï¼šæ·±åº¦å•ä½æ˜¯cmï¼Œéœ€è¦è½¬æ¢ä¸ºç±³
        if depth.dtype == np.uint16:
            # å‡è®¾16ä½æ·±åº¦å›¾ï¼Œå•ä½æ˜¯cmï¼Œè½¬æ¢ä¸ºç±³
            depth_meters = depth.astype(np.float32) / 100.0
        elif depth.dtype == np.uint8:
            # å¦‚æœæ˜¯8ä½å›¾åƒï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
            max_height_cm = 25500  # å‡è®¾æœ€å¤§é«˜åº¦255ç±³(25500cm)
            depth_meters = depth.astype(np.float32) * (max_height_cm / 255.0) / 100.0
        else:
            # å…¶ä»–æ•°æ®ç±»å‹ï¼Œå‡è®¾å·²ç»æ˜¯æ­£ç¡®çš„å•ä½
            depth_meters = depth.astype(np.float32)
        
        return depth_meters
    def create_height_evaluation_mask(self, semantic_mask: np.ndarray) -> np.ndarray:
        """
        åˆ›å»ºç”¨äºé«˜åº¦è¯„ä¼°çš„mask
        åªåŒ…å«æœ‰é«˜åº¦æ„ä¹‰çš„ç±»åˆ«ï¼šLow-vegetation, Building, Tree
        
        Args:
            semantic_mask: è¯­ä¹‰åˆ†å‰²mask (0-6çš„ç±»åˆ«æ ‡ç­¾)
        
        Returns:
            height_mask: äºŒå€¼maskï¼Œ1è¡¨ç¤ºæœ‰é«˜åº¦æ„ä¹‰çš„åŒºåŸŸ
        """
        height_mask = np.zeros_like(semantic_mask, dtype=np.float32)
        
        # åªåŒ…å«æœ‰é«˜åº¦æ„ä¹‰çš„ç±»åˆ«
        for class_id in self.HEIGHT_CLASSES:
            height_mask[semantic_mask == class_id] = 1.0
        
        return height_mask
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        sample_info = self.samples[idx]
        
        # åŠ è½½å›¾åƒå’Œæ·±åº¦
        image = self._load_image(sample_info['image_path'])
        depth = self._load_depth(sample_info['depth_path'])
        
        sample = {
            'image': image,
            'depth': depth,
            'image_path': sample_info['image_path'],
            'depth_path': sample_info['depth_path']
        }
        
        # ğŸ”¥ ç‰¹æ®Šå¤„ç†GAMUSçš„è¯­ä¹‰mask
        if self.load_mask and 'mask_path' in sample_info:
            # åŠ è½½è¯­ä¹‰åˆ†å‰²mask
            semantic_mask = cv2.imread(sample_info['mask_path'], cv2.IMREAD_GRAYSCALE)
            if semantic_mask is None:
                print(f"Warning: Cannot load mask: {sample_info['mask_path']}")
            else:
                # åˆ›å»ºç”¨äºé«˜åº¦è¯„ä¼°çš„mask
                height_mask = self.create_height_evaluation_mask(semantic_mask)
                
                # ä¿å­˜ä¸¤ç§mask
                sample['semantic_mask'] = semantic_mask.astype(np.uint8)  # åŸå§‹è¯­ä¹‰mask
                sample['mask'] = height_mask  # ç”¨äºé«˜åº¦è¯„ä¼°çš„äºŒå€¼mask
                sample['mask_path'] = sample_info['mask_path']
                
                # æ·»åŠ è°ƒè¯•ä¿¡æ¯
                unique_classes = np.unique(semantic_mask)
                valid_pixels = np.sum(height_mask > 0)
                total_pixels = height_mask.size
                coverage = valid_pixels / total_pixels
                
                sample['mask_info'] = {
                    'unique_classes': unique_classes.tolist(),
                    'height_coverage': coverage,
                    'valid_pixels': int(valid_pixels),
                    'total_pixels': int(total_pixels)
                }
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            sample = self.transform(sample)
        
        return sample

class DFC2019Dataset(BaseDepthDataset):
    """DFC2019æ•°æ®é›†"""
    
    def _load_samples(self) -> list:
        # DFC2019çš„æ•°æ®åŠ è½½é€»è¾‘
        split_file = self.data_root / f"{self.split}.txt"
        
        samples = []
        if split_file.exists():
            with open(split_file, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        # å‡è®¾æ ¼å¼ï¼šimage_path depth_path
                        parts = line.split()
                        if len(parts) >= 2:
                            samples.append({
                                'image_path': str(self.data_root / parts[0]),
                                'depth_path': str(self.data_root / parts[1])
                            })
        else:
            # ç›´æ¥ä»ç›®å½•ç»“æ„æ¨æ–­
            image_dir = self.data_root / "images"
            depth_dir = self.data_root / "heights"
            
            for image_path in sorted(image_dir.glob("*.tif")):
                depth_path = depth_dir / f"{image_path.stem}.tif"
                if depth_path.exists():
                    samples.append({
                        'image_path': str(image_path),
                        'depth_path': str(depth_path)
                    })
        
        print(f"Loaded {len(samples)} samples from DFC2019 {self.split}")
        return samples

class VaihingenDataset(BaseDepthDataset):
    """Vaihingenæ•°æ®é›†"""
    
    def _load_samples(self) -> list:
        # Vaihingençš„æ•°æ®åŠ è½½é€»è¾‘
        samples = []
        
        # å‡è®¾å·²ç»é¢„å¤„ç†æˆ512x512çš„patches
        image_dir = self.data_root / self.split / "images"
        depth_dir = self.data_root / self.split / "depths"
        
        for image_path in sorted(image_dir.glob("*.png")):
            depth_path = depth_dir / f"{image_path.stem}.png"
            if depth_path.exists():
                samples.append({
                    'image_path': str(image_path),
                    'depth_path': str(depth_path)
                })
        
        print(f"Loaded {len(samples)} samples from Vaihingen {self.split}")
        return samples
class GoogleHeightDataset(BaseDepthDataset):
    """Google Heightæ•°æ®é›†"""
    
    def _load_samples(self) -> list:
        # GoogleHeightDataçš„æ•°æ®åŠ è½½é€»è¾‘
        split_dir = self.data_root / self.split
        
        # æ•°æ®ç»“æ„ï¼šGoogleHeightData/train/images/ å’Œ GoogleHeightData/train/labels/
        image_dir = split_dir / "images"
        label_dir = split_dir / "labels"
        
        if not image_dir.exists():
            raise FileNotFoundError(f"Image directory not found: {image_dir}")
        if not label_dir.exists():
            raise FileNotFoundError(f"Label directory not found: {label_dir}")
        
        samples = []
        for image_path in sorted(image_dir.glob("*.tif")):
            # å¯¹åº”çš„æ·±åº¦æ–‡ä»¶åº”è¯¥æœ‰ç›¸åŒçš„æ–‡ä»¶å
            label_path = label_dir / image_path.name
            
            if label_path.exists():
                samples.append({
                    'image_path': str(image_path),
                    'depth_path': str(label_path)
                })
        
        print(f"Loaded {len(samples)} samples from GoogleHeightData {self.split}")
        return samples
def get_dataset(dataset_name: str, **kwargs) -> BaseDepthDataset:
    """å·¥å‚å‡½æ•°ï¼šæ ¹æ®åç§°è·å–æ•°æ®é›†"""
    datasets = {
        'GAMUS': GAMUSDataset,
        'DFC2019': DFC2019Dataset, 
        'Vaihingen': VaihingenDataset,
        'GoogleHeight':GoogleHeightDataset,
    }
    
    if dataset_name not in datasets:
        raise ValueError(f"Unknown dataset: {dataset_name}. Available: {list(datasets.keys())}")
    
    return datasets[dataset_name](**kwargs)